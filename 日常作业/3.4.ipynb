import numpy as np
import time

def lossValue(X, y, W):
    return np.sum(np.square(X * W - y)) / (2 * len(y))

def gradient(X, y, W):
    return X.T * (X * W - y) / len(y)

def batch_gradient_descent(X, y, W, alpha, loss_change, max_iter):
    loss = lossValue(X, y, W)
    for i in range(max_iter):
        W = W - alpha * gradient(X, y, W)
        newloss = lossValue(X, y, W)
        if abs(loss - newloss) < loss_change:
            break
        loss = newloss
    return W

def stochastic_gradient_descent(X, y, W, alpha, loss_change, max_iter):
    m = len(y)
    loss = lossValue(X, y, W)
    for i in range(max_iter):
        for j in range(m):
            rand_ind = np.random.randint(0, m)
            x_i = X[rand_ind, :]
            y_i = y[rand_ind, :]
            grad = x_i.T * (x_i * W - y_i)
            W = W - alpha * grad
        newloss = lossValue(X, y, W)
        if abs(loss - newloss) < loss_change:
            break
        loss = newloss
    return W

temperatures = [15, 20, 25, 30, 35, 40]
flowers = [136, 140, 155, 160, 157, 175]

X = np.mat([[1]*len(temperatures), temperatures]).T
y = np.mat(flowers).T

W = np.mat([0.0, 0.0]).T

# Batch Gradient Descent
start_time = time.time()
alpha = 0.00025
loss_change = 0.000001
max_iter = 30000
W_batch = batch_gradient_descent(X, y, W, alpha, loss_change, max_iter)
end_time = time.time()
print("Batch Gradient Descent:")
print("Weights:", W_batch)
print("Time taken:", end_time - start_time, "seconds")

# Stochastic Gradient Descent
start_time = time.time()
alpha = 0.00025
loss_change = 0.000001
max_iter = 3000
W_stochastic = stochastic_gradient_descent(X, y, W, alpha, loss_change, max_iter)
end_time = time.time()
print("\nStochastic Gradient Descent:")
print("Weights:", W_stochastic)
print("Time taken:", end_time - start_time, "seconds")
